'''
Created on Nov 29, 2016

@author: demon
'''
# Loading necessary Libaries
from __future__ import division
import nltk
from nltk.text import Text
from ProjectTokenizer import pToken
from ProjectTagger import pTagger

# Loading all text data
txtFile=open('C:/Users/demon/OneDrive/Documents/GitHub/ANLY_520-51_FA2016/FinalProject/data/AllText.txt','r')
tempTxt = txtFile.read()
txtFile.close()

# Tokenizing the text
pTok=pToken(tempTxt)

# Basic statistics of the text
print("Number of Words : "+str(len(pTok)))
print("Number of Unique Words : "+str(len(set(pTok))))
print("Lexical Diversity : "+str(len(pTok)/len(set(pTok))))

# Converting to nltk.text.Text form to easily create Frequency Distribution 
nText = Text(pTok)
fdistv = nltk.FreqDist(nText)
vocab = fdistv.keys()
#fdistv.tabulate(50)
#fdistv.plot(50,cumulative=True)

#print(vocab[:50])
#print(nText.collocations())

taggedTok = pTagger(vocab)
tags = [t for (w,t) in taggedTok]
#fdisttag=nltk.FreqDist(tags)
#fdisttag.tabulate()
#fdisttag.plot(cumulative=True)
#emojis= [w for (w,t) in taggedTok if t=="emoji"]
#print(emojis)

link= [w for (w,t) in taggedTok if t=="link"]
print(link)

numb= [w for (w,t) in taggedTok if t=="num"]
print(numb)

#hastag= [w for (w,t) in taggedTok if t=="htag"]
#print(hastag)

#ud= [w for (w,t) in taggedTok if t=="UserID"]
#print(ud)

#pun= [w for (w,t) in taggedTok if t=="punct"]
#print(pun)