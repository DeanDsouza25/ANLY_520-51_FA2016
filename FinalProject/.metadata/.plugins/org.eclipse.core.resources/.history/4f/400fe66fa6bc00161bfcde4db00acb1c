'''
@author: Dean D'souza
'''
# Importing Required Libraries
import csv
import nltk
import math
import random
import re
from ProjectTokenizer import pToken
from nltk.corpus import stopwords
from nltk.metrics.association import BigramAssocMeasures

# Empty holder for all tweets
allTweets = []

# Reading in the data from the file
csvfile = open('C:/Users/demon/OneDrive/Documents/GitHub/ANLY_520-51_FA2016/FinalProject/data/EmotionText.csv')
reader = csv.DictReader(csvfile, dialect='excel')
for row in reader:
    allTweets.append((row['content'], row['sentiment']))
csvfile.close()

# Separating based on sentiment
emptySet = [(e,s) for (e,s) in allTweets if s in ['empty']]
neutralSet = [(e,s) for (e,s) in allTweets if s in ['neutral']]
sadnessSet = [(e,s) for (e,s) in allTweets if s in ['sadness']]
loveSet = [(e,s) for (e,s) in allTweets if s in ['love']]
hateSet = [(e,s) for (e,s) in allTweets if s in ['hate']]
surpriseSet = [(e,s) for (e,s) in allTweets if s in ['surprise']]
reliefSet = [(e,s) for (e,s) in allTweets if s in ['relief']]
worrySet = [(e,s) for (e,s) in allTweets if s in ['worry']]

# Defining cutoffs
emCutOff = int(math.floor(len(emptySet)*0.8))
neCutOff = int(math.floor(len(neutralSet)*0.8))
saCutOff = int(math.floor(len(sadnessSet)*0.8))
loCutOff = int(math.floor(len(loveSet)*0.8))
haCutOff = int(math.floor(len(hateSet)*0.8))
suCutOff = int(math.floor(len(surpriseSet)*0.8))
reCutOff = int(math.floor(len(reliefSet)*0.8))
woCutOff = int(math.floor(len(worrySet)*0.8))

# Separating training and testing sets
trainTweets = emptySet[:emCutOff] + neutralSet[:neCutOff] + sadnessSet[:saCutOff] + loveSet[:loCutOff] + hateSet[:haCutOff] + surpriseSet[:suCutOff] + reliefSet[:reCutOff] + worrySet[:woCutOff]
testTweets = emptySet[emCutOff:] + neutralSet[neCutOff:] + sadnessSet[saCutOff:] + loveSet[loCutOff:] + hateSet[haCutOff:] + surpriseSet[suCutOff:] + reliefSet[reCutOff:] + worrySet[woCutOff:]

# Randomizing the order
random.shuffle(trainTweets)
random.shuffle(testTweets)

# Creating a list of Tokenized words and sentiment
tweets = []
for (senten,sentiment) in trainTweets:
    filteredWords=([w.lower() for w in pToken(senten) if len(w) >= 2 and w not in stopwords.words('english')])
    tweets.append((filteredWords,sentiment))

# Function to create list of all words
def TweetWordScores(tweets):
    t_fd = nltk.FreqDist()
    cond_t_fd = nltk.ConditionalFreqDist()
    for (f,s) in tweets:
        for w in f:
            t_fd.inc(w)
            cond_t_fd[s].inc(w)
            
    em_count = cond_t_fd['empty'].N()
    ne_count = cond_t_fd['neutral'].N()
    sa_count = cond_t_fd['sadness'].N()
    lo_count = cond_t_fd['love'].N()
    ha_count = cond_t_fd['hate'].N()
    su_count = cond_t_fd['surprise'].N()
    re_count = cond_t_fd['relief'].N()
    wo_count = cond_t_fd['worry'].N()
    
    total_count = em_count+ne_count+sa_count+lo_count+ha_count+su_count+re_count+wo_count
    
    scores = {}
    for w,f in t_fd.iteritems():
        em_score = BigramAssocMeasures.chi_sq(cond_t_fd['empty'][w], (f,em_count), total_count)
        ne_score = BigramAssocMeasures.chi_sq(cond_t_fd['neutral'][w], (f,ne_count), total_count)
        sa_score = BigramAssocMeasures.chi_sq(cond_t_fd['sadness'][w], (f,sa_count), total_count)
        lo_score = BigramAssocMeasures.chi_sq(cond_t_fd['love'][w], (f,lo_count), total_count)
        ha_score = BigramAssocMeasures.chi_sq(cond_t_fd['hate'][w], (f,ha_count), total_count)
        su_score = BigramAssocMeasures.chi_sq(cond_t_fd['surprise'][w], (f,su_count), total_count)
        re_score = BigramAssocMeasures.chi_sq(cond_t_fd['relief'][w], (f,re_count), total_count)
        wo_score = BigramAssocMeasures.chi_sq(cond_t_fd['worry'][w], (f,wo_count), total_count)
        scores[w] = em_score+ne_score+sa_score+lo_score+ha_score+su_score+re_score+wo_score 
    
    return scores

def bestWords(sc,num):
    bestVals=sorted(sc.iteritems(), key=lambda (w,s):s, reverse=True)[:num]
    bestW=set([w for w in bestVals])
    return bestW
    
tweetsScores = bestWords(TweetWordScores(tweets),1000)

# Function to extract features from new tweets
def FeatExt(ttweet):
    tokWords = [w.lower() for w in pToken(ttweet) if len(w) >= 2 and w not in stopwords.words('english')]
    feat = {}
    for e in tokWords:
        if e in tweetsScores:
            feat['contains(%s)' % str(e)] = e
    feat['htag'] = 0
    feat['punct'] = 0
    for e in tokWords:
        if re.search('^#[A-Za-z0-9_\-\+\.]+$',e):
            feat['htag'] = feat['htag']+ 1
        if re.search('^[\.\*\?\,\!\&/\^\`\{\|\}\~\"\$\#\'\:\s]+$',e):
            feat['punct'] = feat['punct'] +len(e)
    return feat

# Preparing data for building and testing Naive Bayes Classifier
trainTweetsApplied = nltk.classify.apply_features(FeatExt, trainTweets)
testTweetsApplied = nltk.classify.apply_features(FeatExt, testTweets) 

# Building the Naive Bayes classifier
classifierNB = nltk.NaiveBayesClassifier.train(trainTweetsApplied)

# Preparing data for evaluation
refSet = {}
testFeatSet = {}
for i, (feature,sentiment) in enumerate(testTweetsApplied):
    refSet[sentiment].add[i]
    pred= classifierNB.classify(feature)
    testFeatSet[pred].add(i)

# Most Informative features
print(classifierNB.show_most_informative_features(30))

# Evaluating
print(nltk.classify.accuracy(classifierNB,testTweetsApplied))
print("For empty tag:")
print(nltk.metrics.precision(refSet['empty'], testFeatSet['empty']))
print(nltk.metrics.recall(refSet['empty'], testFeatSet['empty']))
