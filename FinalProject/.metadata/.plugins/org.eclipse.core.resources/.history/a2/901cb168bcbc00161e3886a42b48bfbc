'''
Created on Nov 29, 2016

@author: demon
'''
# Loading necessary Libaries
from __future__ import division
import nltk
from nltk.text import Text
from nltk.corpus import stopwords
from ProjectTokenizer import pToken
from ProjectTagger import pTagger

# Loading all text data
txtFile=open('C:/Users/demon/OneDrive/Documents/GitHub/ANLY_520-51_FA2016/FinalProject/data/AllText.txt','r')
tempTxt = txtFile.read()
txtFile.close()

# Size of the file
print(len(tempTxt))

# Tokenizing the text
pToktemp=pToken(tempTxt)

# Basic statistics of the text
#print("Number of Words : "+str(len(pToktemp)))
#print("Number of Unique Words : "+str(len(set(pToktemp))))
#print("Lexical Diversity : "+str(len(pToktemp)/len(set(pToktemp))))

# After removing stop words
pTok = [w for w in pToktemp if w not in stopwords.words('english')]

# Basic statistics of the text
#print("Number of Words : "+str(len(pTok)))
#print("Number of Unique Words : "+str(len(set(pTok))))
#print("Lexical Diversity : "+str(len(pTok)/len(set(pTok))))


# Converting to nltk.text.Text form to easily create Frequency Distribution 
nText = Text(pTok)
fdistv = nltk.FreqDist(nText)
vocab = fdistv.keys()

# List of top most tokens
#print(vocab[:50])

# Cummulative Frequency Plot of the top most tokens
fdistv.plot(50)
#fdistv.plot(50,cumulative=True)

# Collocations (a.k.a. Words occuring together with a frequency considered greater than chance)
print(nText.collocations())

# Tagging the tokens with Part-of-Speech tag
taggedTok = pTagger(vocab)
tags = [t for (w,t) in taggedTok]

# Frequency Distribution of tags
fdisttag=nltk.FreqDist(tags)
#fdisttag.tabulate()

# Frequency Distribution Plot of tags
fdisttag.plot()
